{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Wild Blueberry Yield Prediction\n\n\"The dataset used for predictive modeling was generated by the Wild Blueberry Pollination Simulation Model, which is an open-source, spatially-explicit computer simulation program that enables exploration of how various factors, including plant spatial arrangement, outcrossing and self-pollination, bee species compositions and weather conditions, in isolation and combination, affect pollination efficiency and yield of the wild blueberry agroecosystem. The simulation model has been validated by the field observation and experimental data collected in Maine USA and Canadian Maritimes during the last 30 years and now is a useful tool for hypothesis testing and theory development for wild blueberry pollination researches.\"\n\nThe aim it to predict blueberry yield\n\nFeatures Unit Description:\n- Clonesize m2 The average blueberry clone size in the field\n- Honeybee bees/m2/min Honeybee density in the field\n- Bumbles bees/m2/min Bumblebee density in the field\n- Andrena bees/m2/min Andrena bee density in the field\n- Osmia bees/m2/min Osmia bee density in the field\n- MaxOfUpperTRange ℃ The highest record of the upper band daily air temperature during the bloom season\n- MinOfUpperTRange ℃ The lowest record of the upper band daily air temperature\n- AverageOfUpperTRange ℃ The average of the upper band daily air temperature\n- MaxOfLowerTRange ℃ The highest record of the lower band daily air temperature\n- MinOfLowerTRange ℃ The lowest record of the lower band daily air temperature\n- AverageOfLowerTRange ℃ The average of the lower band daily air temperature\n- RainingDays Day The total number of days during the bloom season, each of which has precipitation larger than zero\n- AverageRainingDays Day The average of raining days of the entire bloom season\n- 'fruitset', 'fruitmass', 'seeds' **non** defined\n\nResources:\n- [Kaggle challenge](https://www.kaggle.com/competitions/playground-series-s3e14/overview)","metadata":{}},{"cell_type":"code","source":"# importing standard libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve, LearningCurveDisplay, ShuffleSplit\n\nfrom xgboost import XGBRegressor\n\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:59:39.006071Z","iopub.execute_input":"2023-05-11T13:59:39.007789Z","iopub.status.idle":"2023-05-11T13:59:41.991475Z","shell.execute_reply.started":"2023-05-11T13:59:39.007687Z","shell.execute_reply":"2023-05-11T13:59:41.990315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Seaborn theme parameters\ntheme_parameters =  {\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'grid.alpha':0.3,\n    'axes.titlesize': 16,\n    'figure.figsize': (12, 4),\n}\n\n# Set the theme\nsns.set_theme(style='whitegrid',\n              palette=sns.color_palette('colorblind'), \n              rc=theme_parameters)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:59:44.192661Z","iopub.execute_input":"2023-05-11T13:59:44.193626Z","iopub.status.idle":"2023-05-11T13:59:44.200110Z","shell.execute_reply.started":"2023-05-11T13:59:44.193583Z","shell.execute_reply":"2023-05-11T13:59:44.199051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"read_from_kaggle = True","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:59:47.230594Z","iopub.execute_input":"2023-05-11T13:59:47.231773Z","iopub.status.idle":"2023-05-11T13:59:47.238249Z","shell.execute_reply.started":"2023-05-11T13:59:47.231725Z","shell.execute_reply":"2023-05-11T13:59:47.236669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if read_from_kaggle:\n    \n    df_train = pd.read_csv(\"/kaggle/input/playground-series-s3e14/train.csv\")\n    df_test = pd.read_csv(\"/kaggle/input/playground-series-s3e14/test.csv\")\n    \nelse:\n\n    df_train = pd.read_csv(\"./../data/blueberry_train.csv\")\n    df_test = pd.read_csv(\"./../data/blueberry_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:18:16.164626Z","iopub.execute_input":"2023-05-11T14:18:16.165502Z","iopub.status.idle":"2023-05-11T14:18:16.255702Z","shell.execute_reply.started":"2023-05-11T14:18:16.165456Z","shell.execute_reply":"2023-05-11T14:18:16.254526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:59:51.726300Z","iopub.execute_input":"2023-05-11T13:59:51.727018Z","iopub.status.idle":"2023-05-11T13:59:51.770927Z","shell.execute_reply.started":"2023-05-11T13:59:51.726984Z","shell.execute_reply":"2023-05-11T13:59:51.769782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:59:54.732000Z","iopub.execute_input":"2023-05-11T13:59:54.732956Z","iopub.status.idle":"2023-05-11T13:59:54.961744Z","shell.execute_reply.started":"2023-05-11T13:59:54.732918Z","shell.execute_reply":"2023-05-11T13:59:54.960489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-05-11T13:59:57.703790Z","iopub.execute_input":"2023-05-11T13:59:57.704264Z","iopub.status.idle":"2023-05-11T13:59:57.788400Z","shell.execute_reply.started":"2023-05-11T13:59:57.704228Z","shell.execute_reply":"2023-05-11T13:59:57.787284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop id column\ndf_train = df_train.drop(columns='id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:15:05.585699Z","iopub.execute_input":"2023-05-11T14:15:05.586150Z","iopub.status.idle":"2023-05-11T14:15:05.593489Z","shell.execute_reply.started":"2023-05-11T14:15:05.586115Z","shell.execute_reply":"2023-05-11T14:15:05.592469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis - EDA","metadata":{}},{"cell_type":"markdown","source":"## Features distribution train vs validation set KDEs","metadata":{}},{"cell_type":"code","source":"figure, ax = plt.subplots(4, 4, figsize=(16, 12))\nax = ax.flatten()\n\nfor index, col_name in enumerate(df_train.columns[:-1]):\n    \n    sns.kdeplot(data=df_train[col_name],\n                label='Train',\n                ax=ax[index])\n    \n    sns.kdeplot(data=df_test[col_name],\n                label='Test',\n                ax=ax[index])\n   \n    ax[index].set_title(col_name, fontsize=14)\n    \n    ax[index].tick_params(labelrotation=45)\n    \n    # Retrieve legend information\n    handles = ax[index].get_legend_handles_labels()[0]\n    labels = ax[index].get_legend_handles_labels()[1]\n    ax[index].legend().remove()\n\n# Set the legend\nfigure.legend(handles, \n              labels, \n              loc='upper center', \n              bbox_to_anchor=(0.5, 1.03), \n              fontsize=12,\n              ncol=3)\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:15:10.027205Z","iopub.execute_input":"2023-05-11T14:15:10.028417Z","iopub.status.idle":"2023-05-11T14:15:19.647746Z","shell.execute_reply.started":"2023-05-11T14:15:10.028372Z","shell.execute_reply":"2023-05-11T14:15:19.646403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The distribution of the features is consistend between train and validation sets","metadata":{}},{"cell_type":"code","source":"figure, ax = plt.subplots(1, 2)\nax = ax.flatten()\n\nsns.boxplot(data=df_train, \n            y='honeybee',\n            ax=ax[0])\n\nsns.boxplot(data=df_test,\n            y='honeybee',\n            ax=ax[1],\n            color='orange')\n\nax[0].set_title('Train honeybee distribution')\n\nax[1].set_title('Test honeybee distribution')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:15:43.093504Z","iopub.execute_input":"2023-05-11T14:15:43.094014Z","iopub.status.idle":"2023-05-11T14:15:43.506701Z","shell.execute_reply.started":"2023-05-11T14:15:43.093983Z","shell.execute_reply":"2023-05-11T14:15:43.505605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['honeybee'].value_counts().sort_index(ascending=False).head(5)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:15:47.795919Z","iopub.execute_input":"2023-05-11T14:15:47.796380Z","iopub.status.idle":"2023-05-11T14:15:47.810624Z","shell.execute_reply.started":"2023-05-11T14:15:47.796347Z","shell.execute_reply":"2023-05-11T14:15:47.809290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Outlyers of honeybees need to be dropped before modeling (just on df_train)","metadata":{}},{"cell_type":"code","source":"# dropping 'honeybee' values >= 5 in test\ndf_train = df_train[df_train['honeybee'] < 5].reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:16:27.626709Z","iopub.execute_input":"2023-05-11T14:16:27.628163Z","iopub.status.idle":"2023-05-11T14:16:27.637368Z","shell.execute_reply.started":"2023-05-11T14:16:27.628103Z","shell.execute_reply":"2023-05-11T14:16:27.635603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label distribution KDE","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(data=df_train['yield'],\n            label='Train')\n\nplt.title('Label distribution (yield)')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:17:08.890378Z","iopub.execute_input":"2023-05-11T14:17:08.891515Z","iopub.status.idle":"2023-05-11T14:17:09.386617Z","shell.execute_reply.started":"2023-05-11T14:17:08.891463Z","shell.execute_reply":"2023-05-11T14:17:09.385630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the label yield looks similar to the distribution of the features 'fruitset', 'fruitmass' and 'seeds'. Since these features are not described (see notebook intro) we can hypotesize that they have been inferred from the label yield.","metadata":{}},{"cell_type":"markdown","source":"## Pearson Correlation","metadata":{}},{"cell_type":"code","source":"# Generate correlation matrix\ncorrelation_train = df_train.corr(method='pearson')","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:10:18.377242Z","iopub.execute_input":"2023-05-09T13:10:18.377590Z","iopub.status.idle":"2023-05-09T13:10:18.400696Z","shell.execute_reply.started":"2023-05-09T13:10:18.377562Z","shell.execute_reply":"2023-05-09T13:10:18.399797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a mask for the upper triangle\ncorrelation_mask = np.triu(np.ones_like(correlation_train, dtype=bool))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:10:18.401973Z","iopub.execute_input":"2023-05-09T13:10:18.403194Z","iopub.status.idle":"2023-05-09T13:10:18.408232Z","shell.execute_reply.started":"2023-05-09T13:10:18.403160Z","shell.execute_reply":"2023-05-09T13:10:18.407192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure, ax = plt.subplots(figsize=(14, 10))\n\nax = sns.heatmap(correlation_train, \n            mask=correlation_mask, \n            cmap='mako',\n            vmax=1.0, \n            vmin=-1.0, \n            center=0, \n            square=True, \n            linewidths=.5, \n            annot=True,\n            annot_kws={'fontsize': 8},\n            cbar_kws={\"shrink\":.8, 'orientation':'vertical'})\n\n\nax.set_title('Correlation heatmap')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:10:18.409522Z","iopub.execute_input":"2023-05-09T13:10:18.409928Z","iopub.status.idle":"2023-05-09T13:10:19.433812Z","shell.execute_reply.started":"2023-05-09T13:10:18.409898Z","shell.execute_reply":"2023-05-09T13:10:19.432909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The correlation matrix shows a very similar and a high correlation behaviour between the features 'fruitset', 'fruitmass' and 'seeds and the lable 'yield', thus the above hypotesis was quite likely correct\n- The feature 'clonesize' and 'honeybee' looks to be positively correlated (0.85), we can hypotesize that honeybees are attracted to blueberry cultivation with higher clonesize (maybe because the domestication of honeybees). Anyway, this doesn't appear to translate into a highet yield\n- The temperature measures are duplicated, it's enough to keep just one\n- Same with AverageRainingDays and RainingDays. Dropping AverageRainingDays","metadata":{}},{"cell_type":"markdown","source":" ## Pairplot\n \n Plotting pairplots between main features to look at potential non-linear relaationships","metadata":{}},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:10:19.435091Z","iopub.execute_input":"2023-05-09T13:10:19.435415Z","iopub.status.idle":"2023-05-09T13:10:19.442455Z","shell.execute_reply.started":"2023-05-09T13:10:19.435387Z","shell.execute_reply":"2023-05-09T13:10:19.441273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df_train[['clonesize', 'honeybee', 'bumbles', 'andrena', 'osmia', 'MaxOfUpperTRange', 'AverageRainingDays', 'fruitset', 'fruitmass', 'seeds', 'yield']],\n             kind=\"reg\",\n             diag_kind='kde',\n             plot_kws={'line_kws':{'color':'red'}},\n             corner=True)\n\nplt.suptitle('Features and Target Pairplots', \n             fontsize=20, \n             fontweight='bold')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:10:19.443892Z","iopub.execute_input":"2023-05-09T13:10:19.444229Z","iopub.status.idle":"2023-05-09T13:11:37.399128Z","shell.execute_reply.started":"2023-05-09T13:10:19.444193Z","shell.execute_reply":"2023-05-09T13:11:37.397982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data are probably AI generated. Doesn't look like there are particular non-linar relationships between features.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing data before modeling","metadata":{}},{"cell_type":"markdown","source":"## Get the datasets model-ready","metadata":{}},{"cell_type":"code","source":"# dropping 'honeybee' values >= 5 in test\ndf_train = df_train[df_train['honeybee'] < 5].reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:18:38.006932Z","iopub.execute_input":"2023-05-11T14:18:38.007415Z","iopub.status.idle":"2023-05-11T14:18:38.020041Z","shell.execute_reply.started":"2023-05-11T14:18:38.007383Z","shell.execute_reply":"2023-05-11T14:18:38.018607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_engineered_features(data: pd.DataFrame) -> pd.DataFrame:\n    \n    \"\"\"\n    Create engineered features to have the dataset model-ready\n    \n    Args:\n        data Pandas.DataFrame input\n    \n    Returns:\n        data Pandas.DataFrame with engineered features\n    \"\"\"\n    \n    # Create a feature clonesize per honeybee\n    data['clonesize per honeybee'] = data['clonesize'] * data['honeybee']\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:18:40.610075Z","iopub.execute_input":"2023-05-11T14:18:40.611871Z","iopub.status.idle":"2023-05-11T14:18:40.617950Z","shell.execute_reply.started":"2023-05-11T14:18:40.611832Z","shell.execute_reply":"2023-05-11T14:18:40.616740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the train data model-ready\ndf_train = compute_engineered_features(df_train.copy())\n\n# get the test data model-ready\ndf_test = compute_engineered_features(df_test.copy())","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:18:43.099396Z","iopub.execute_input":"2023-05-11T14:18:43.099819Z","iopub.status.idle":"2023-05-11T14:18:43.108793Z","shell.execute_reply.started":"2023-05-11T14:18:43.099788Z","shell.execute_reply":"2023-05-11T14:18:43.107698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining features and label","metadata":{}},{"cell_type":"code","source":"features = ['clonesize', \n            'honeybee', \n            'bumbles', \n            'andrena', \n            'osmia', \n            'AverageOfUpperTRange',\n            'AverageRainingDays', \n            'fruitset', \n            'fruitmass', \n            'seeds',\n            'clonesize per honeybee']\n\nlabel = ['yield']","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:15.761852Z","iopub.execute_input":"2023-05-11T14:19:15.762241Z","iopub.status.idle":"2023-05-11T14:19:15.767565Z","shell.execute_reply.started":"2023-05-11T14:19:15.762213Z","shell.execute_reply":"2023-05-11T14:19:15.766642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling data","metadata":{}},{"cell_type":"code","source":"# Scaling train data with StandardScaler\nscaler = StandardScaler(copy=True,\n                        with_mean=True,\n                        with_std=True)\n\nscaler.fit(df_train[features])\n\ndf_train_scaled = pd.DataFrame(scaler.transform(df_train[features]))\n\n# Scaling removed column names - put them back\ndf_train_scaled.columns = features","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:18.425728Z","iopub.execute_input":"2023-05-11T14:19:18.426153Z","iopub.status.idle":"2023-05-11T14:19:18.444897Z","shell.execute_reply.started":"2023-05-11T14:19:18.426121Z","shell.execute_reply":"2023-05-11T14:19:18.443624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling test data\ndf_test_scaled = pd.DataFrame(scaler.transform(df_test[features]))\n\n# Scaling removed column names - put them back\ndf_test_scaled.columns = features","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:21.302344Z","iopub.execute_input":"2023-05-11T14:19:21.302812Z","iopub.status.idle":"2023-05-11T14:19:21.313944Z","shell.execute_reply.started":"2023-05-11T14:19:21.302778Z","shell.execute_reply":"2023-05-11T14:19:21.312816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data between train and test set","metadata":{}},{"cell_type":"code","source":"# defining a seed\nseed = 108","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:24.254802Z","iopub.execute_input":"2023-05-11T14:19:24.255275Z","iopub.status.idle":"2023-05-11T14:19:24.261741Z","shell.execute_reply.started":"2023-05-11T14:19:24.255244Z","shell.execute_reply":"2023-05-11T14:19:24.260408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define X and y for the training set\nX = df_train_scaled\ny = df_train.iloc[:, -2]\n\n# Splitting train dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:28.036339Z","iopub.execute_input":"2023-05-11T14:19:28.036815Z","iopub.status.idle":"2023-05-11T14:19:28.048441Z","shell.execute_reply.started":"2023-05-11T14:19:28.036781Z","shell.execute_reply":"2023-05-11T14:19:28.047090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Define metric(s) to compare the models\nmetrics = ['RMSE']\n\n# Initialize DataFrame of model performance\nperformance = pd.DataFrame(columns=metrics)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:31.925042Z","iopub.execute_input":"2023-05-11T14:19:31.925498Z","iopub.status.idle":"2023-05-11T14:19:31.933900Z","shell.execute_reply.started":"2023-05-11T14:19:31.925464Z","shell.execute_reply":"2023-05-11T14:19:31.932515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Instantiate a LinearRegression model\nmodel_lr = LinearRegression()\n\n# Fit logreg to the train set\nmodel_lr.fit(X_train, y_train)\n\n# predict y_pred values\ny_pred_lr = model_lr.predict(X_test)\n\n# Compute RMSE metric\nrmse_lr = round(mean_squared_error(y_test, y_pred_lr) ** 0.5, 2)\n\nprint('RMSE: {}'.format(rmse_lr))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:59:13.803389Z","iopub.execute_input":"2023-05-10T07:59:13.803802Z","iopub.status.idle":"2023-05-10T07:59:13.847845Z","shell.execute_reply.started":"2023-05-10T07:59:13.803769Z","shell.execute_reply":"2023-05-10T07:59:13.846068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['Linear Regression'] = [rmse_lr]","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:19:52.395109Z","iopub.execute_input":"2023-05-09T14:19:52.395504Z","iopub.status.idle":"2023-05-09T14:19:52.401706Z","shell.execute_reply.started":"2023-05-09T14:19:52.395460Z","shell.execute_reply":"2023-05-09T14:19:52.400628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Instantiate rf\nmodel_rf = RandomForestRegressor(n_estimators=25,\n                                 random_state=seed)\n            \n# Fit rf to the training set    \nmodel_rf.fit(X_train, y_train) \n\n# Predict the test set labels\ny_pred_rf = model_rf.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_rf = round(mean_squared_error(y_test, y_pred_rf) ** 0.5, 2)\n\n# Print rmse_test\nprint('RMSE: {}'.format(rmse_rf))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:19:54.748213Z","iopub.execute_input":"2023-05-09T14:19:54.748951Z","iopub.status.idle":"2023-05-09T14:19:56.002357Z","shell.execute_reply.started":"2023-05-09T14:19:54.748916Z","shell.execute_reply":"2023-05-09T14:19:56.000976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['Random Forest Regressor'] = [rmse_rf]","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:19:59.055653Z","iopub.execute_input":"2023-05-09T14:19:59.057835Z","iopub.status.idle":"2023-05-09T14:19:59.065194Z","shell.execute_reply.started":"2023-05-09T14:19:59.057789Z","shell.execute_reply":"2023-05-09T14:19:59.063897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Instantiate gb\nmodel_gb = GradientBoostingRegressor(max_depth=4, \n                                     n_estimators=200,\n                                     random_state=2)\n\n# Fit gb to the training set\nmodel_gb.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred_gb = model_gb.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_gb = round(mean_squared_error(y_test, y_pred_gb) ** 0.5, 2)\n\n# Print rmse_test\nprint('RMSE: {}'.format(rmse_gb))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:20:02.298038Z","iopub.execute_input":"2023-05-09T14:20:02.298720Z","iopub.status.idle":"2023-05-09T14:20:05.446881Z","shell.execute_reply.started":"2023-05-09T14:20:02.298683Z","shell.execute_reply":"2023-05-09T14:20:05.445906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['Gradient Boosting Regressor'] = [rmse_gb]","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:20:05.448534Z","iopub.execute_input":"2023-05-09T14:20:05.449077Z","iopub.status.idle":"2023-05-09T14:20:05.454871Z","shell.execute_reply.started":"2023-05-09T14:20:05.449046Z","shell.execute_reply":"2023-05-09T14:20:05.453911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"# define XGBoost hyperparams dict\nhyperparams_xgb = {\n    'n_estimators' : 500,\n    'max_depth' : 5,\n    'learning_rate' : 0.01\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:48.084891Z","iopub.execute_input":"2023-05-11T14:19:48.085417Z","iopub.status.idle":"2023-05-11T14:19:48.090782Z","shell.execute_reply.started":"2023-05-11T14:19:48.085373Z","shell.execute_reply":"2023-05-11T14:19:48.089721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\nmodel_xgb = XGBRegressor(**hyperparams_xgb)\n\n# Train the xgb to the training set\nmodel_xgb.fit(X_train, y_train)\n\n# Get predictions\ny_pred_xgb = model_xgb.predict(X_test)\n\n# Compute metrics\nrmse_xgb = round(mean_squared_error(y_test, y_pred_xgb) ** 0.5, 2)\n\nprint('RMSE: {}'.format(rmse_xgb))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:50.231722Z","iopub.execute_input":"2023-05-11T14:19:50.232835Z","iopub.status.idle":"2023-05-11T14:19:52.920514Z","shell.execute_reply.started":"2023-05-11T14:19:50.232789Z","shell.execute_reply":"2023-05-11T14:19:52.919598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['XGBoost Regressor'] = [rmse_xgb]","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:19:53.345565Z","iopub.execute_input":"2023-05-11T14:19:53.348427Z","iopub.status.idle":"2023-05-11T14:19:53.356387Z","shell.execute_reply.started":"2023-05-11T14:19:53.348380Z","shell.execute_reply":"2023-05-11T14:19:53.355009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"# define LightGBM hyperparams dict\nhyperparams_lgb = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'force_row_wise': True,\n    'n_estimators': 1000,\n    'verbose': 0\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:20:22.628161Z","iopub.execute_input":"2023-05-09T14:20:22.628656Z","iopub.status.idle":"2023-05-09T14:20:22.634667Z","shell.execute_reply.started":"2023-05-09T14:20:22.628622Z","shell.execute_reply":"2023-05-09T14:20:22.633297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Define the model\nmodel_lgb = lgb.LGBMRegressor(**hyperparams_lgb)\n\n# Fit model to the training set\nmodel_lgb.fit(X_train, np.ravel(y_train))\n\n# Get predictions\ny_pred_lgb = model_lgb.predict(X_test)\n\n# Compute metrics\nrmse_lgb = round(mean_squared_error(y_test, y_pred_lgb) ** 0.5, 2)\n\nprint('RMSE: {}'.format(rmse_lgb))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:20:24.972814Z","iopub.execute_input":"2023-05-09T14:20:24.973198Z","iopub.status.idle":"2023-05-09T14:20:32.300737Z","shell.execute_reply.started":"2023-05-09T14:20:24.973169Z","shell.execute_reply":"2023-05-09T14:20:32.299733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['LightGBM Regressor'] = [rmse_lgb]","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:20:32.302114Z","iopub.execute_input":"2023-05-09T14:20:32.302668Z","iopub.status.idle":"2023-05-09T14:20:32.309078Z","shell.execute_reply.started":"2023-05-09T14:20:32.302635Z","shell.execute_reply":"2023-05-09T14:20:32.308289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model comparison","metadata":{}},{"cell_type":"code","source":"# Sort dataframe by the metric\nperformance.sort_values('RMSE', inplace=True)\n\n# Plot model metrics\nax = sns.barplot(data=performance, \n                 x='RMSE', \n                 y=performance.index.tolist())\n\nax.set_title('Model comparison')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:20:39.925904Z","iopub.execute_input":"2023-05-09T14:20:39.926281Z","iopub.status.idle":"2023-05-09T14:20:40.266940Z","shell.execute_reply.started":"2023-05-09T14:20:39.926254Z","shell.execute_reply":"2023-05-09T14:20:40.265933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning best sklearn model hyperparameters","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Define a dictionary containing the parameters to tune\nparams_model_gb = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 4, 5],\n}\n\n# Instantiate grid_gb\ngrid_model_gb = GridSearchCV(estimator=GradientBoostingRegressor(random_state=seed),\n                       param_grid=params_model_gb,\n                       scoring='neg_mean_squared_error',\n                       cv=3,\n                       verbose=1,\n                       n_jobs=-1)\n\n# Fit grid_model_rf\ngrid_model_gb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:20:48.439725Z","iopub.execute_input":"2023-05-09T14:20:48.440122Z","iopub.status.idle":"2023-05-09T14:23:49.372116Z","shell.execute_reply.started":"2023-05-09T14:20:48.440092Z","shell.execute_reply":"2023-05-09T14:23:49.371109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract best hyperparams\nbest_params_gb = grid_model_gb.best_params_\n\nprint('Best hyperparameters for Gradient Boosting Regressor:\\n', best_params_gb)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:23:49.374639Z","iopub.execute_input":"2023-05-09T14:23:49.375579Z","iopub.status.idle":"2023-05-09T14:23:49.381453Z","shell.execute_reply.started":"2023-05-09T14:23:49.375536Z","shell.execute_reply":"2023-05-09T14:23:49.380332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the best model performance\nbest_model_gb = grid_model_gb.best_estimator_\n\n# Fit best_model_gb to the training set\nbest_model_gb.fit(X_train, y_train)\n\n# Predict test set labels using the best model\ny_pred_best_gb = best_model_gb.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_best_gb = round(mean_squared_error(y_test, y_pred_best_gb) ** 0.5, 2)\n\n# Print rmse_test\nprint('RMSE best gb: {}'.format(rmse_best_gb))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:24:42.523610Z","iopub.execute_input":"2023-05-09T14:24:42.524001Z","iopub.status.idle":"2023-05-09T14:24:44.115109Z","shell.execute_reply.started":"2023-05-09T14:24:42.523972Z","shell.execute_reply":"2023-05-09T14:24:44.113957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['Tuned Gradient Boosting Regressor'] = [rmse_best_gb]","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:24:49.494641Z","iopub.execute_input":"2023-05-09T14:24:49.495047Z","iopub.status.idle":"2023-05-09T14:24:49.502436Z","shell.execute_reply.started":"2023-05-09T14:24:49.495019Z","shell.execute_reply":"2023-05-09T14:24:49.501188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning XGBoost model","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Define a dictionary containing the parameters to tune\nparams_model_xgb = {\n    'n_estimators': [55, 250, 500],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 4, 5],\n}\n\n# Instantiate grid_gb\ngrid_model_xgb = GridSearchCV(estimator=XGBRegressor(random_state=seed),\n                       param_grid=params_model_xgb,\n                       scoring='neg_mean_squared_error',\n                       cv=3,\n                       verbose=1,\n                       n_jobs=-1)\n\n# Fit grid_model_rf\ngrid_model_xgb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:20:06.225227Z","iopub.execute_input":"2023-05-11T14:20:06.225705Z","iopub.status.idle":"2023-05-11T14:20:53.348944Z","shell.execute_reply.started":"2023-05-11T14:20:06.225673Z","shell.execute_reply":"2023-05-11T14:20:53.347638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract best hyperparams\nbest_params_xgb = grid_model_xgb.best_params_\n\nprint('Best hyperparameters for XGBoost model:\\n', best_params_xgb)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:20:53.351682Z","iopub.execute_input":"2023-05-11T14:20:53.352945Z","iopub.status.idle":"2023-05-11T14:20:53.358948Z","shell.execute_reply.started":"2023-05-11T14:20:53.352901Z","shell.execute_reply":"2023-05-11T14:20:53.357758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the best model performance\nbest_model_xgb = grid_model_xgb.best_estimator_\n\n# Fit best_model_gb to the training set\nbest_model_xgb.fit(X_train, y_train)\n\n# Predict test set labels using the best model\ny_pred_best_xgb = best_model_xgb.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_best_xgb = round(mean_squared_error(y_test, y_pred_best_xgb) ** 0.5, 2)\n\n# Print rmse_test\nprint('RMSE best xgb: {}'.format(rmse_best_xgb))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:21:01.852119Z","iopub.execute_input":"2023-05-11T14:21:01.852622Z","iopub.status.idle":"2023-05-11T14:21:02.097634Z","shell.execute_reply.started":"2023-05-11T14:21:01.852586Z","shell.execute_reply":"2023-05-11T14:21:02.096627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['Tuned XGBoost Regressor'] = [rmse_best_xgb]","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:21:07.036741Z","iopub.execute_input":"2023-05-11T14:21:07.037822Z","iopub.status.idle":"2023-05-11T14:21:07.045831Z","shell.execute_reply.started":"2023-05-11T14:21:07.037785Z","shell.execute_reply":"2023-05-11T14:21:07.044136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Updating model comparison after tuning the better performing models","metadata":{}},{"cell_type":"code","source":"# Sort dataframe by the metric\nperformance.sort_values('RMSE', inplace=True)\n\n# Plot model metrics\nax = sns.barplot(data=performance, \n                 x='RMSE', \n                 y=performance.index.tolist())\n\nax.set_title('Model comparison updated')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T14:33:24.192048Z","iopub.execute_input":"2023-05-09T14:33:24.192456Z","iopub.status.idle":"2023-05-09T14:33:24.540366Z","shell.execute_reply.started":"2023-05-09T14:33:24.192427Z","shell.execute_reply":"2023-05-09T14:33:24.539121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Diagnostic\n\nDiagnostic the fit of the best performing models\n - (tuned) xgboost regressor\n - sklearn (tuned) gradient boosting regressor","metadata":{}},{"cell_type":"markdown","source":"## Feature Importance","metadata":{"execution":{"iopub.status.busy":"2023-05-08T15:49:17.720809Z","iopub.execute_input":"2023-05-08T15:49:17.721268Z","iopub.status.idle":"2023-05-08T15:49:17.752456Z","shell.execute_reply.started":"2023-05-08T15:49:17.721235Z","shell.execute_reply":"2023-05-08T15:49:17.751367Z"}}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n\n# Compute the feature importance\nimportances_xgb = sorted(list(zip(best_model_xgb.feature_names_in_,\n                              best_model_xgb.feature_importances_)))\n\n# Transform it into a DataFrame\nimportances_xgb_df = pd.DataFrame(importances_xgb,\n                                     columns= ['Feature', 'Importance'])\n\n# Sorting importances\nimportances_xgb_df = importances_xgb_df.sort_values(['Importance'], ascending=False).reset_index(drop=True)\n\n# Plot the feature importance\nax[0] = sns.barplot(data=importances_xgb_df, \n                 x='Feature', \n                 y='Importance',\n                 ax=ax[0])\nax[0].set_title('XGBoost Regressor Feature Importance')\nax[0].tick_params(labelrotation=90)\n\n# Compute the feature importance\nimportances_gb = sorted(list(zip(best_model_gb.feature_names_in_,\n                              best_model_gb.feature_importances_)))\n\n# Transform it into a DataFrame\nimportances_gb_df = pd.DataFrame(importances_gb,\n                                     columns= ['Feature', 'Importance'])\n\n# Sorting importances\nimportances_gb_df = importances_gb_df.sort_values(['Importance'], ascending=False).reset_index(drop=True)\n\n# Plot the feature importance\nax[1] = sns.barplot(data=importances_gb_df, \n                 x='Feature', \n                 y='Importance',\n                 ax=ax[1])\nax[1].set_title('Gradient Boosting Regressor Feature Importance')\nax[1].tick_params(labelrotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T15:37:09.943399Z","iopub.execute_input":"2023-05-09T15:37:09.943828Z","iopub.status.idle":"2023-05-09T15:37:10.617516Z","shell.execute_reply.started":"2023-05-09T15:37:09.943797Z","shell.execute_reply":"2023-05-09T15:37:10.616557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning curve","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(16,4))\n\ncommon_params = {\n    \"X\": X,\n    \"y\": y,\n    \"train_sizes\": np.linspace(0.1, 1.0, 10),\n    \"cv\": ShuffleSplit(n_splits=10, test_size=0.2, random_state=0),\n    \"score_type\": \"both\",\n    \"n_jobs\": 4,\n    \"line_kw\": {\"marker\": \"o\"},\n    \"std_display_style\": \"fill_between\",\n    \"score_name\": \"Score\",\n}\n\nfor ax_idx, estimator in enumerate([model_xgb, best_model_gb]):\n    \n    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n    \n    handles, label = ax[ax_idx].get_legend_handles_labels()\n    \n    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n    \n    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")\n    \nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T16:34:13.340607Z","iopub.execute_input":"2023-05-09T16:34:13.341777Z","iopub.status.idle":"2023-05-09T16:42:02.646114Z","shell.execute_reply.started":"2023-05-09T16:34:13.341726Z","shell.execute_reply":"2023-05-09T16:42:02.644850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Residuals","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(14,4))\n\n# Create a DataFrame of residuals for best_model_xgb\nresiduals_xgb = y_test - y_pred_best_xgb\ndf_resid_xgb = pd.DataFrame({'residuals': residuals_xgb, 'y_pred_best_xgb': y_pred_best_xgb})\n\n# Plot the residuals for best_model_xgb\nsns.residplot(x='y_pred_best_xgb',\n              y='residuals', \n              data=df_resid_xgb, \n              ax=ax[0])\nax[0].set_title('XGBoost Regressor Residual Plot', fontsize=16)\nax[0].set_xlabel('Predicted values')\nax[0].set_ylabel('')\n\n# Create a DataFrame of residuals for best_model_gb\nresiduals_bg = y_test - y_pred_best_gb\ndf_resid_bg = pd.DataFrame({'residuals': residuals_bg, 'y_pred_best_gb': y_pred_best_gb})\n\n# Plot the residuals for best_model_gb\nsns.residplot(x='y_pred_best_gb', \n              y='residuals', \n              data=df_resid_bg, \n              ax=ax[1])\nax[1].set_title('Gradient Boosting Regressor Residual Plot', fontsize=16)\nax[1].set_xlabel('Predicted values')\nax[1].set_ylabel('Residuals')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T15:38:19.374454Z","iopub.execute_input":"2023-05-09T15:38:19.375074Z","iopub.status.idle":"2023-05-09T15:38:20.012092Z","shell.execute_reply.started":"2023-05-09T15:38:19.375039Z","shell.execute_reply":"2023-05-09T15:38:20.010890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q-Q Plot","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n\n# Calculate the residuals for best_model_xgb\nresiduals_xgb = y_test - y_pred_best_xgb\n\n# Generate a QQ plot for best_model_xgb\nstats.probplot(residuals_xgb, \n               plot=ax[0])\nax[0].set_title('Q-Q Plot of XGBoost Regressor Residuals', fontsize=16)\nax[0].set_xlabel('Theoretical quantiles')\nax[0].set_ylabel('')\n\n# Calculate the residuals for best_model_gb\nresiduals_gb = y_test - y_pred_best_gb\n\n# Generate a QQ plot for model 1\nstats.probplot(residuals_gb, \n               plot=ax[1])\nax[1].set_title('Q-Q Plot of Gradient Boosting Regressor Residuals', fontsize=16)\nax[1].set_xlabel('Theoretical quantiles')\nax[1].set_ylabel('Sample quantiles')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T18:22:28.480348Z","iopub.execute_input":"2023-05-09T18:22:28.480730Z","iopub.status.idle":"2023-05-09T18:22:29.073172Z","shell.execute_reply.started":"2023-05-09T18:22:28.480701Z","shell.execute_reply":"2023-05-09T18:22:29.072235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Actual Values vs Predicted Values","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n\n# Plot the predicted vs actual values for each model\nax[0].scatter(y_test, \n            y_pred_best_gb, \n            label='GB - RMSE: {}'.format(rmse_best_gb))\nax[0].plot([y_test.min(), y_test.max()], \n         [y_test.min(), y_test.max()], \n         'k--', \n         lw=2)\n\nax[1].scatter(y_test, \n            y_pred_best_xgb, \n            label='XGB - RMSE: {}'.format(rmse_best_xgb))\nax[1].plot([y_test.min(), y_test.max()], \n         [y_test.min(), y_test.max()], \n         'k--', \n         lw=2)\n\n# Add a legend to the plot\nax[0].legend()\nax[1].legend()\n\n# Set x and y axis labels for the plot\nax[0].set_xlabel('Actual values')\nax[0].set_ylabel('Predicted values')\nax[1].set_xlabel('Actual values')\nax[1].set_ylabel('Predicted values')\n\nfig.suptitle('Predicted vs Actual Values')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T18:21:59.959693Z","iopub.execute_input":"2023-05-09T18:21:59.960081Z","iopub.status.idle":"2023-05-09T18:22:00.653484Z","shell.execute_reply.started":"2023-05-09T18:21:59.960052Z","shell.execute_reply":"2023-05-09T18:22:00.652270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Diagnostic considerations\n\nfrom the various diagnostic plot we see that the models behave similarly.\n\n- They both slightly overfit the trainig set (disproportion among features contribution and steeper learning curve for the training set against the test test)\n- Both models are have quite poor performance in predicting extreme values (see Q-Q plots and Predicted vs Actual values plots)","metadata":{}},{"cell_type":"markdown","source":"# Challenge Submission","metadata":{"execution":{"iopub.status.busy":"2023-05-10T15:30:17.570501Z","iopub.execute_input":"2023-05-10T15:30:17.571455Z","iopub.status.idle":"2023-05-10T15:30:17.575448Z","shell.execute_reply.started":"2023-05-10T15:30:17.571419Z","shell.execute_reply":"2023-05-10T15:30:17.574568Z"}}},{"cell_type":"code","source":"# Generate predictions with test set\nsubmission_predictions = best_model_xgb.predict(df_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:21:19.687013Z","iopub.execute_input":"2023-05-11T14:21:19.687442Z","iopub.status.idle":"2023-05-11T14:21:19.710262Z","shell.execute_reply.started":"2023-05-11T14:21:19.687413Z","shell.execute_reply":"2023-05-11T14:21:19.709164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_scaled.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:21:23.215394Z","iopub.execute_input":"2023-05-11T14:21:23.215807Z","iopub.status.idle":"2023-05-11T14:21:23.239455Z","shell.execute_reply.started":"2023-05-11T14:21:23.215777Z","shell.execute_reply":"2023-05-11T14:21:23.237740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission DataFrame\nsubmission = pd.DataFrame({'id': df_test.id, 'yield': submission_predictions})","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:21:31.253312Z","iopub.execute_input":"2023-05-11T14:21:31.253791Z","iopub.status.idle":"2023-05-11T14:21:31.262306Z","shell.execute_reply.started":"2023-05-11T14:21:31.253755Z","shell.execute_reply":"2023-05-11T14:21:31.260631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:21:35.502302Z","iopub.execute_input":"2023-05-11T14:21:35.502808Z","iopub.status.idle":"2023-05-11T14:21:35.515757Z","shell.execute_reply.started":"2023-05-11T14:21:35.502772Z","shell.execute_reply":"2023-05-11T14:21:35.514367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write CSV File\nsubmission.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:26:34.409797Z","iopub.execute_input":"2023-05-11T14:26:34.410308Z","iopub.status.idle":"2023-05-11T14:26:34.453311Z","shell.execute_reply.started":"2023-05-11T14:26:34.410272Z","shell.execute_reply":"2023-05-11T14:26:34.451894Z"},"trusted":true},"execution_count":null,"outputs":[]}]}